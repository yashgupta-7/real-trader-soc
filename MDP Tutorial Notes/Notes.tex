\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-1in}
\addtolength{\textheight}{1.75in}

\usepackage{graphicx}
\graphicspath{ {./Desktop/R-ea-LTrader-SoC} }

\title{R(ea)L Trader SoC Notes}
\author{Shubh Kumar}
\date{IIT-B, Summer of Code 2021}

\begin{document}

\maketitle

\section{Markov Decision Process}
\begin{itemize}
  \item MDPs provides us a way to Mathematically formualte the typical RL Problem.
  \item \textbf{The Agent-Environment Relationship}:
  \begin{itemize}
    \item Actions can be any decision we want the agent to learn and state can be anything which can be useful in choosing actions.
    \item Not everything in the environment is unknown to the agent, but the agent can't change anything arbitarily.
    \item The Agent could only interact through its actions.
    \item The agent-environment relationship represents the limit of the agent control and not itâ€™s knowledge.
  \end{itemize}
  \item \textbf{The Markov Property}: Future is independent of the past given the present.      \\
  \begin{center}
    $P[S_{t+1} | S_{t}] = P[ S | S_{1}, ......, S_{t}]$  \\~\\
    $S[i]$ denotes the states, and $t$ is like time(present), so all the equation says is that its Probability of Transition is independent of the past states and even if we have the information pertaining to that, it won't change the Probability of current Transition which would be determined solely by our current state. \\~\\
  \end{center}

  State Transition Probability: Probability of moving from one state to the next. It could be abstracted as a matrix with each row containing the probabilities for a particular state, all of which (row-entries) sum to 1.

  \item \textbf{Markov Process/Chains}: Sequence of Random states which fulfill the Markov Property.

  \item  Rewards are the numerical values that the agent receives on performing some action at some state(s) in the environment.
  \item This total sum of reward the agent receives from the environment is called returns. \\
  \begin{center}
    \[ G_{t} = \sum_{i = t+1}^{T}{r_{i}} \] \\~\\
    $t$ is time. A single action, has its rewards reaped not just at the end of that action, but till the end of a particular number of further actions as well(perhaps a weighted sum would be more accurate! Coming up next ;) ) \\~\\
  \end{center}

  \item Episodic Tasks: Those which have an end, Continuous Tasks: Those which don't
  \item In order to compensate for the summing up to infinity(as we don't want to do that!), We have the Discount Factor($\gamma$). It's a value between 0 and 1. 0 means only consider the current reward for deciding the current action. 1 means keep considering it till infinity. Thus, a value between 0.2 and 0.8 is considered good.
  \item With that in mind, we change our expression for $G_t$ to:
  \begin{center}
     \[ G_{t} =  \sum_{i = 0}^{ \infty }{ \gamma^{i} R_{t+1+i}} \]
  \end{center}

  \item Deciding on Discount Factor: It depends on the task that we want to train an agent for. (Hyper-parameterization)

  \item \textbf{Markov Reward Process}:
  \begin{center}
    $\mathbf{R_s} = E[R_{t+1} | S_{t} ]$    \\~\\
  \end{center}
  Mathematically, we may define MRP as (S,P,R, $\gamma$), where: \\
  \begin{itemize}
    \item S is set of States
    \item P is the Transition-Probability matrix
    \item R is the reward function
    \item $\gamma$ is the discount factor
  \end{itemize}

  \item \textbf{Policy Function and Value Function}: Value Function determines how good it is for the agent to be in a particular state. A policy defines what actions to perform in a particular state s.

  \item A policy ($\pi$) is a simple function, that defines a probability distribution over Actions $(a \in A)$ for each state $(s \in S)$.   \\
  \begin{center}
    $\pi (a|s) = \mathbb{P} [A_{t} = a | S_{t} = s]$
  \end{center}

  \item The Value of a state, maybe reached at by: \\
  \begin{center}
    $v_{\pi}(s) = \mathbb{E}_{\pi}[G_{t} | S_t = s] = \mathbb{E}_{\pi} $   $\Big[  \sum_{i = 0}^{ \infty }{ \gamma^{i} R_{t+1+i} } | S_{t} = s \Big]$  , for all $s \in S$. \\~\\
  \end{center}

  This equation gives us the expected returns starting from state(s) and going to successor states thereafter, with the policy $\pi$. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic.

  \item Bellman Equation helps us to find optimal policies and value function.
  \item The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.
  \item The key linear equation is: $v = (I - \gamma P)^{-1}R$, where v is the value-vector, I, the identity matrix, $\gamma$, the Discount factor, P, the Transition-Probability Matrix and R the Reward Vector
  \item Markov Decision Process : It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.
  \item MDP could be Mathematically descirbed by S(set of States), A(set of Actions), P(Probability Transition Matrix), R(Rewards accumulated by actions of agents), $\gamma$ (Discount Factor).
  \item State-action value function or Q-Function: This function specifies the how good it is for the agent to take action (a) in a state (s) with a policy $\pi$.
\end{itemize}


\section {Bellman Equation and Optimality}

\begin{itemize}
  \item Bellman's Expectation Equation for value function, helps in determining the value of a state: \( v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \)
  \item Bellman's Expectaion Equation for the State-Action Value function would be: \( q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \)
  \item The above two equations are recursive in the sense, that they provide us with a method to calculate the value function as well as the state-value-action function, provided the values for pre-exisiting states.
  \item Considering only a shallow bottom-top dependence(of depth 1) between $v_{\pi}(s)$ and $q_{\pi}(s,a)$ we get:  \\
  \begin{center}
    \( v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a) \)
  \end{center}
  \item Similarly, if we try to write out a relationship the other way round, and taking into account that there will also be a reward for every action which should be taken into account:  \\
  \begin{center}
    \( q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right) \)
  \end{center}
  \item Now, combining both of them for $v_{\pi}(s)$, we'll get: \\
  \begin{center}
    \( v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right) \)
  \end{center}
  \item And for $q_{\pi}(s,a)$, we'll get:
  \begin{center}
    \( q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right) \)
  \end{center}

  \item Now, we've found the formulation to Bellman Expectation Equation. We still need to figure how to optimize it.
  \item Now we yearn to get the Optimal Value and the Optimal Policy Function.
  \item Mathematically, the Optimal State-Value Function would look like:  \\
    \begin{center}
      \( v_{*}(s)=\max _{\pi} v_{\pi}(s) \)
    \end{center}
  \item And Similarly for the Optimal State-Action Value Function (Q-Function), We'll get \\
  \begin{center}
    \( q_{*}(s, a)=\max _{\pi} q_{\pi}(s, a) \)
  \end{center}
  \item A crucial expression while trying to get to the value function was $\pi (a|s)$ .i.e. The Conditional Probability for us to go forward with the action $a$, while in state $s$. We could prove that a greedy approach is the way to go and thus, we could take this function as:  \\
  \begin{center}
  \begin{equation*}
      \pi (a|s)=\begin{cases}
         1  \quad &\text{if} \, \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{*}(s, a) \\
         0 \quad &\text{if} \, \text{else} \\
   \end{cases}
\end{equation*}
  \end{center}

  \item These notions take us Bellman's Optimality Equation which is what we'll be using, it can be expressed as: \\

  \begin{center}
    \( v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right) \) \\~\\
    \( q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right) \)
  \end{center}

\end{itemize}

\section{Solving MDP using DP}

\begin{itemize}
  \item If you want, you may learn about DP from any standard Algorithms' Text.
  \item The optimal Substructure which characterizes DP arises due to the Recursive Nature of Bellman's Optimality equation.
  \item Solving MDP will have two main parts:
  \begin{enumerate}
    \item \textbf{Prediction:} Predict the optimal Actions using Bellman's Equation. This would essentially mean calculating the Value Function.
    \item \textbf{Control:} Optimizing the value function, we calculated during the prediction process. We find optimal value function and optimal policy for our Markov Decision Process.
  \end{enumerate}
  \item To evaluate a particular Policy, we use the Equation: \\
  \begin{center}
    \( v_{k+1}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right) \) \\~\\
  \end{center}
  where $k$ denotes the iteration we're presently in. As it turns out in the last iteration we'll get to the optimal value Function for each state.
  \item Next, we calculate our $q_{\pi} (s,a)$ corresponding to the new value functions, and we change our Policy greedily, as:
  \begin{center}
  \( \pi^{\prime}(s)=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{\pi}(s, a) \)
  \end{center}
  \item It is provable, that at each step, we'd be getting to a better version of our policy and eventually get the optimal one.
  \item This gives us:  \\ \( q_{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in \mathcal{A}} q_{\pi}(s, a)=q_{\pi}(s, \pi(s))=v_{\pi}(s) \) and.. \\~\\
  \( v_{\pi}(s)=\max _{a \in \mathcal{A}} q_{\pi}(s, a) \)

  \item There are two more variations we should keep in mind:
  \begin{enumerate}
    \item \textbf{Modified Policy Iteration :}  It turns out that after a few iterations we already have achieved our optimal policy. We donâ€™t need to wait for our value function to converge. We can say if the value function gets updated by a very little amount, say less than an epsilon then we can stop this process. To get the exact conditions when we are to stop, We have the \textbf{Principle of Optimality}, which says that a Policy is optimal if we have for it:
    \begin{itemize}
      \item  An Optimal first step, say A
      \item And then The Steps from the next step onwards, lead to little change in our Value Function
      \item Thus, a policy is said to be optimal if it behaves optimally after we have taken one optimal step, say action A.
    \end{itemize}
    \item \textbf{Value Iteration :} Value iteration is the special case for Policy Iteration. When the process of policy evaluation is stopped after one step. We put the values into the Bellman Optimal Equation, until we find the Optimal Value Function. We use only this optimal Value Function in order to calculate the Optimal Policy. \\
    The Bellman Optimal Equation as its used in Value Iteration could be written as:
    \begin{center}
      \( v_{k+1}(s)=\max _{a \in \mathcal{A}}\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right) \)
    \end{center}
  \end{enumerate}
\end{itemize}

\end{document}
